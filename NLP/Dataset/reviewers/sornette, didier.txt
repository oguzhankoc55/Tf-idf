{fenge}
physics;0210130	foreshocks explained by cascades of triggered seismicity	the observation of foreshocks preceding large earthquakes and the suggestionthat foreshocks have specific properties that may be used to distinguish themfrom other earthquakes have raised the hope that large earthquakes may bepredictable. among proposed anomalous properties are the larger proportion thannormal of large versus small foreshocks, the power law acceleration ofseismicity rate as a function of time to the mainshock and the spatialmigration of foreshocks toward the mainshock, when averaging over manysequences. using southern california seismicity, we show that these propertiesand others arise naturally from the simple model that any earthquake maytrigger other earthquakes, without arbitrary distinction between foreshocks,aftershocks and mainshocks. we find that foreshocks precursory properties areindependent of the mainshock size. this implies that earthquakes (large orsmall) are predictable to the same degree as seismicity rate is predictablefrom past seismicity by taking into account cascades of triggering. thecascades of triggering give rise naturally to long-range and long-timeinteractions, which can explain the observations of correlations in seismicityover surprisingly large length scales.
{fenge}
physics;0505079	fundamental factors versus herding in the 2000-2005 us stock market and  prediction	we present a general methodology to incorporate fundamental economic factorsto our previous theory of herding to describe bubbles and antibubbles. we startfrom the strong form of rational expectation and derive the general method toincorporate factors in addition to the log-periodic power law (lppl) signatureof herding developed in ours and others' works. these factors include interestrate, interest spread, historical volatility, implied volatility and exchangerates. standard statistical aic and wilks tests allow us to compare theexplanatory power of the different proposed factor models. we find that thehistorical volatility played the key role before august of 2002. around october2002, the interest rate dominated. in the first six months of 2003, the foreignexchange rate became the key factor. since the end of 2003, all factors haveplayed an increasingly large role. however, the most surprising result is thatthe best model is the second-order lppl without any factor. we thus present ascenario for the future evolution of the us stock market based on theextrapolation of the fit of the second-order lppl formula, which suggests thatherding is still the dominating force and that the unraveling of the us stockmarket antibubble since 2000 is still qualitatively similar to (butquantitatively different from) the japanese nikkei case after 1990.
{fenge}
physics;0607197	lead-lag cross-sectional structure and detection of  correlated-anticorrelated regime shifts: application to the volatilities of  inflation and economic growth rates	we have recently introduced the ``thermal optimal path'' (top) method toinvestigate the real-time lead-lag structure between two time series. the topmethod consists in searching for a robust noise-averaged optimal path of thedistance matrix along which the two time series have the greatest similarity.here, we generalize the top method by introducing a more general definition ofdistance which takes into account possible regime shifts between positive andnegative correlations. this generalization to track possible changes ofcorrelation signs is able to identify possible transitions from one convention(or consensus) to another. numerical simulations on synthetic time seriesverify that the new top method performs as expected even in the presence ofsubstantial noise. we then apply it to investigate changes of convention in thedependence structure between the historical volatilities of the usa inflationrate and economic growth rate. several measures show that the new top methodsignificantly outperforms standard cross-correlation methods.
{fenge}
physics;0701171	a case study of speculative financial bubbles in the south african stock  market 2003-2006	we tested 45 indices and common stocks traded in the south african stockmarket for the possible existence of a bubble over the period from jan. 2003 tomay 2006. a bubble is defined by a faster-than-exponential acceleration withsignificant log-periodic oscillations. the faster-than-exponential accelerationcharacteristics are tested with several different metrics, includingnonlinearity on the logarithm of the price and power law fits. the log-periodicproperties are investigated in detail using the first-order log-periodicpower-law (lppl) formula, the parametric detrending method, the$(h,q)$-analysis, and the second-order weierstrass-type model, resulting in aconsistent and robust estimation of the fundamental angular log-frequency$\omega_1 =7\pm 2$, in reasonable agreement with previous estimations on manyother bubbles in developed and developing markets. sensitivity tests of theestimated critical times and of the angular log-frequency are performed byvarying the first date and the last date of the stock price time series. thesetests show that the estimated parameters are robust. with the insight of 6additional month of data since the analysis was performed, we observe that manyof the stocks on the south africa market experienced an abrupt drop mid-june2006, which is compatible with the predicted $t_c$ for several of the stocks,but not all. this suggests that the mini-crash that occurred around mid-june of2006 was only a partial correction, which has resumed into a renewed bubblyacceleration bound to end some times in 2007, similarly to what happened on thes&amp;p500 us market from oct. 1997 to aug. 1998.
{fenge}
physics;0703084	automatic reconstruction of fault networks from seismicity catalogs: 3d  optimal anisotropic dynamic clustering	we propose a new pattern recognition method that is able to reconstruct the3d structure of the active part of a fault network using the spatial locationof earthquakes. the method is a generalization of the so-called dynamicclustering method, that originally partitions a set of datapoints intoclusters, using a global minimization criterion over the spatial inertia ofthose clusters. the new method improves on it by taking into account the fullspatial inertia tensor of each cluster, in order to partition the dataset intofault-like, anisotropic clusters. given a catalog of seismic events, the outputis the optimal set of plane segments that fits the spatial structure of thedata. each plane segment is fully characterized by its location, size andorientation. the main tunable parameter is the accuracy of the earthquakelocalizations, which fixes the resolution, i.e. the residual variance of thefit. the resolution determines the number of fault segments needed to describethe earthquake catalog, the better the resolution, the finer the structure ofthe reconstructed fault segments. the algorithm reconstructs successfully thefault segments of synthetic earthquake catalogs. applied to the real catalogconstituted of a subset of the aftershocks sequence of the 28th june 1992landers earthquake in southern california, the reconstructed plane segmentsfully agree with faults already known on geological maps, or with blind faultsthat appear quite obvious on longer-term catalogs. future improvements of themethod are discussed, as well as its potential use in the multi-scale study ofthe inner structure of fault zones.
{fenge}
0904.0944	gravity-driven instabilities: interplay between state-and-velocity  dependent frictional sliding and stress corrosion damage cracking	we model the progressive maturation of a heterogeneous mass towards agravity-driven instability, characterized by the competition between frictionalsliding and tension cracking, using array of slider blocks on an inclined basalsurface, which interact via elastic-brittle springs. a realistic state- andrate-dependent friction law describes the block-surface interaction. the innermaterial damage occurs via stress corrosion. three regimes, controlling themass instability and its precursory behavior, are classified as a function ofthe ratio $t_c/t_f$ of two characteristic time scales associated with internaldamage/creep and with frictional sliding. for $t_c/t_f \gg 1$, the whole massundergoes a series of internal stick and slip events, associated with aninitial slow average downward motion of the whole mass, and progressivelyaccelerates until a global coherent runaway is observed. for $t_c/t_f \ll 1$,creep/damage occurs sufficiently fast compared with nucleation of sliding,causing bonds to break, and the bottom part of the mass undergoes afragmentation process with the creation of a heterogeneous population ofsliding blocks. for the intermediate regime $t_c/t_f \sim 1$, a macroscopiccrack nucleates and propagates along the location of the largest curvatureassociated with the change of slope from the stable frictional state in theupper part to the unstable frictional sliding state in the lower part. theother important parameter is the young modulus $y$ which controls thecorrelation length of displacements in the system.
{fenge}
0905.0220	financial bubbles, real estate bubbles, derivative bubbles, and the  financial and economic crisis	the financial crisis of 2008, which started with an initially well-definedepicenter focused on mortgage backed securities (mbs), has been cascading intoa global economic recession, whose increasing severity and uncertain durationhas led and is continuing to lead to massive losses and damage for billions ofpeople. heavy central bank interventions and government spending programs havebeen launched worldwide and especially in the usa and europe, with the hope tounfreeze credit and boltster consumption. here, we present evidence andarticulate a general framework that allows one to diagnose the fundamentalcause of the unfolding financial and economic crisis: the accumulation ofseveral bubbles and their interplay and mutual reinforcement has led to anillusion of a "perpetual money machine" allowing financial institutions toextract wealth from an unsustainable artificial process. taking stock of thisdiagnostic, we conclude that many of the interventions to address the so-calledliquidity crisis and to encourage more consumption are ill-advised and evendangerous, given that precautionary reserves were not accumulated in the "goodtimes" but that huge liabilities were. the most "interesting" present timesconstitute unique opportunities but also great challenges, for which we offer afew recommendations.
{fenge}
0907.4290	dragon-kings, black swans and the prediction of crises	we develop the concept of ``dragon-kings'' corresponding to meaningfuloutliers, which are found to coexist with power laws in the distributions ofevent sizes under a broad range of conditions in a large variety of systems.these dragon-kings reveal the existence of mechanisms of self-organization thatare not apparent otherwise from the distribution of their smaller siblings. wepresent a generic phase diagram to explain the generation of dragon-kings anddocument their presence in six different examples (distribution of city sizes,distribution of acoustic emissions associated with material failure,distribution of velocity increments in hydrodynamic turbulence, distribution offinancial drawdowns, distribution of the energies of epileptic seizures inhumans and in model animals, distribution of the earthquake energies). weemphasize the importance of understanding dragon-kings as being oftenassociated with a neighborhood of what can be called equivalently a phasetransition, a bifurcation, a catastrophe (in the sense of rene thom), or atipping point. the presence of a phase transition is crucial to learn how todiagnose in advance the symptoms associated with a coming dragon-king. severalexamples of predictions using the derived log-periodic power law method arediscussed, including material failure predictions and the forecasts of the endof financial bubbles.
{fenge}
0909.1007	bubble diagnosis and prediction of the 2005-2007 and 2008-2009 chinese  stock market bubbles	by combining (i) the economic theory of rational expectation bubbles, (ii)behavioral finance on imitation and herding of investors and traders and (iii)the mathematical and statistical physics of bifurcations and phase transitions,the log-periodic power law model has been developed as a flexible tool todetect bubbles. the lppl model considers the faster-than-exponential (power lawwith finite-time singularity) increase in asset prices decorated byaccelerating oscillations as the main diagnostic of bubbles. it embodies apositive feedback loop of higher return anticipations competing with negativefeedback spirals of crash expectations. we use the lppl model in one of itsincarnations to analyze two bubbles and subsequent market crashes in twoimportant indexes in the chinese stock markets between may 2005 and july 2009.both the shanghai stock exchange composite and shenzhen stock exchangecomponent indexes exhibited such behavior in two distinct time periods: 1) frommid-2005, bursting in oct. 2007 and 2) from nov. 2008, bursting in thebeginning of aug. 2009. we successfully predicted time windows for both crashesin advance with the same methods used to successfully predict the peak inmid-2006 of the us housing bubble and the peak in july 2008 of the global oilbubble. the more recent bubble in the chinese indexes was detected and its endor change of regime was predicted independently by two groups with similarresults, showing that the model has been well-documented and can be replicatedby industrial practitioners. here we present more detailed analysis of theindividual chinese index predictions and of the methods used to make and testthem.
{fenge}
0911.1921	diagnostics of rational expectation financial bubbles with stochastic  mean-reverting termination times	we propose two rational expectation models of transient financial bubbleswith heterogeneous arbitrageurs and positive feedbacks leading toself-reinforcing transient stochastic faster-than-exponential price dynamics.as a result of the nonlinear feedbacks, the termination of a bubble is found tobe characterized by a finite-time singularity in the bubble price formationprocess ending at some potential critical time $\tilde{t}_c$, which follows amean-reversing stationary dynamics. because of the heterogeneity of therational agents' expectations, there is a synchronization problem for theoptimal exit times determined by these arbitrageurs, which leads to thesurvival of the bubble almost all the way to its theoretical end time. theexplicit exact analytical solutions of the two models provide nonlineartransformations which allow us to develop novel tests for the presence ofbubbles in financial time series. avoiding the difficult problem of parameterestimation of the stochastic differential equation describing the pricedynamics, the derived operational procedures allow us to diagnose bubbles thatare in the making and to forecast their termination time. the tests performedon three financial markets, the us s&amp;p500 index from 1 february 1980 to 31october 2008, the us nasdaq composite index from 1 january 1980 to 31 july 2008and the hong kong hang seng index from 1 december 1986 to 30 november 2008,suggest the feasibility of advance bubble warning.
{fenge}
1001.0265	diagnosis and prediction of tipping points in financial markets: crashes  and rebounds	by combining (i) the economic theory of rational expectation bubbles, (ii)behavioral finance on imitation and herding of investors and traders and (iii)the mathematical and statistical physics of bifurcations and phase transitions,the log-periodic power law (lppl) model has been developed as a flexible toolto detect bubbles. the lppl model considers the faster-than-exponential (powerlaw with finite-time singularity) increase in asset prices decorated byaccelerating oscillations as the main diagnostic of bubbles. it embodies apositive feedback loop of higher return anticipations competing with negativefeedback spirals of crash expectations. the power of the lppl model isillustrated by two recent real-life predictions performed recently by ourgroup: the peak of the oil price bubble in early july 2008 and the burst of abubble on the shanghai stock market in early august 2009. we then present theconcept of "negative bubbles", which are the mirror images of positive bubbles.we argue that similar positive feedbacks are at work to fuel these accelerateddownward price spirals. we adapt the lppl model to these negative bubbles andimplement a pattern recognition method to predict the end times of the negativebubbles, which are characterized by rebounds (the mirror images of crashesassociated with the standard positive bubbles). the out-of-sample testsquantified by error diagrams demonstrate the high significance of theprediction performance.
{fenge}
1002.1070	the lehman brothers effect and bankruptcy cascades	inspired by the bankruptcy of lehman brothers and its consequences on theglobal financial system, we develop a simple model in which the lehman defaultevent is quantified as having an almost immediate effect in worsening thecredit worthiness of all financial institutions in the economic network. in ourstylized description, all properties of a given firm are captured by itseffective credit rating, which follows a simple dynamics of co-evolution withthe credit ratings of the other firms in our economic network. the dynamicsresembles the evolution of potts spin-glass with external global fieldcorresponding to a panic effect in the economy. the existence of a global phasetransition, between paramagnetic and ferromagnetic phases, explains the largesusceptibility of the system to negative shocks. we show that bailing out thefirst few defaulting firms does not solve the problem, but does have the effectof alleviating considerably the global shock, as measured by the fraction offirms that are not defaulting as a consequence. this beneficial effect is thecounterpart of the large vulnerability of the system of coupled firms, whichare both the direct consequences of the collective self-organized endogenousbehaviors of the credit ratings of the firms in our economic network.
{fenge}
1003.2882	exuberant innovation: the human genome project	we present a detailed synthesis of the development of the human genomeproject (hgp) from 1986 to 2003 in order to test the "social bubble" hypothesisthat strong social interactions between enthusiastic supporters of the hgpweaved a network of reinforcing feedbacks that led to a widespread endorsementand extraordinary commitment by those involved in the project, beyond whatwould be rationalized by a standard cost-benefit analysis in the presence ofextraordinary uncertainties and risks. the vigorous competition and racebetween the initially public project and several private initiatives is arguedto support the social bubble hypothesis. we also present quantitative analysesof the concomitant financial bubble concentrated on the biotech sector.confirmation of this hypothesis is offered by the present consensus that itwill take decades to exploit the fruits of the hgp, via a slow and arduousprocess aiming at disentangling the extraordinary complexity of the humancomplex body. the hgp has ushered other initiatives, based on the recognitionthat there is much that genomics cannot do, and that "the future belongs toproteomics". we present evidence that the competition between the public andprivate sector actually played in favor of the former, since its financialburden as well as its horizon was significantly reduced (for a long timeagainst its will) by the active role of the later. this suggests thatgovernments can take advantage of the social bubble mechanism to catalyzelong-term investments by the private sector, which would not otherwise besupported.
{fenge}
1003.5926	diagnosis and prediction of market rebounds in financial markets	we introduce the concept of "negative bubbles" as the mirror image ofstandard financial bubbles, in which positive feedback mechanisms may lead totransient accelerating price falls. to model these negative bubbles, we adaptthe johansen-ledoit-sornette (jls) model of rational expectation bubbles with ahazard rate describing the collective buying pressure of noise traders. theprice fall occurring during a transient negative bubble can be interpreted asan effective random downpayment that rational agents accept to pay in the hopeof profiting from the expected occurrence of a possible rally. we validate themodel by showing that it has significant predictive power in identifying thetimes of major market rebounds. this result is obtained by using a generalpattern recognition method which combines the information obtained at multipletimes from a dynamical calibration of the jls model. error diagrams, bayesianinference and trading strategies suggest that one can extract genuineinformation and obtain real skill from the calibration of negative bubbles withthe jls model. we conclude that negative bubbles are in general predictablyassociated with large rebounds or rallies, which are the mirror images of thecrashes terminating standard bubbles.
{fenge}
1006.0885	segmentation of fault networks determined from spatial clustering of  earthquakes	we present a new method of data clustering applied to earthquake catalogs,with the goal of reconstructing the seismically active part of fault networks.we first use an original method to separate clustered events from uncorrelatedseismicity using the distribution of volumes of tetrahedra defined by closestneighbor events in the original and randomized seismic catalogs. the spatialdisorder of the complex geometry of fault networks is then taken into accountby defining faults as probabilistic anisotropic kernels, whose structures aremotivated by properties of discontinuous tectonic deformation and previousempirical observations of the geometry of faults and of earthquake clusters atmany spatial and temporal scales. combining this a priori knowledge withinformation theoretical arguments, we propose the gaussian mixture approachimplemented in an expectation-maximization (em) procedure. a cross-validationscheme is then used and allows the determination of the number of kernels thatshould be used to provide an optimal data clustering of the catalog. thisthree-steps approach is applied to a high quality relocated catalog of theseismicity following the 1986 mount lewis ($m_l=5.7$) event in california andreveals that events cluster along planar patches of about 2 km$^2$, i.e.comparable to the size of the main event. the finite thickness of thoseclusters (about 290 m) suggests that events do not occur on well-definedeuclidean fault core surfaces, but rather that the damage zone surroundingfaults may be seismically active at depth. finally, we propose a connectionbetween our methodology and multi-scale spatial analysis, based on thederivation of spatial fractal dimension of about 1.8 for the set of hypocentersin the mnt lewis area, consistent with recent observations on relocatedcatalogs.
{fenge}
1007.2420	prediction	this chapter first presents a rather personal view of some different aspectsof predictability, going in crescendo from simple linear systems tohigh-dimensional nonlinear systems with stochastic forcing, which exhibitemergent properties such as phase transitions and regime shifts. then, adetailed correspondence between the phenomenology of earthquakes, financialcrashes and epileptic seizures is offered. the presented statistical evidenceprovides the substance of a general phase diagram for understanding the manyfacets of the spatio-temporal organization of these systems. a key insight isto organize the evidence and mechanisms in terms of two summarizing measures:(i) amplitude of disorder or heterogeneity in the system and (ii) level ofcoupling or interaction strength among the system's components. on the basis ofthe recently identified remarkable correspondence between earthquakes andseizures, we present detailed information on a class of stochastic pointprocesses that has been found to be particularly powerful in describingearthquake phenomenology and which, we think, has a promising future inepileptology. the so-called self-exciting hawkes point processes captureparsimoniously the idea that events can trigger other events, and theircascades of interactions and mutual influence are essential to understand thebehavior of these systems.
{fenge}
1007.4104	quantification of deviations from rationality with heavy-tails in human  dynamics	the dynamics of technological, economic and social phenomena is controlled byhow humans organize their daily tasks in response to both endogenous andexogenous stimulations. queueing theory is believed to provide a generic answerto account for the often observed power-law distributions of waiting timesbefore a task is fulfilled. however, the general validity of the power law andthe nature of other regimes remain unsettled. using anonymized data collectedby google at the world wide web level, we identify the existence of severaladditional regimes characterizing the time required for a population ofinternet users to execute a given task after receiving a message. depending onthe under- or over-utilization of time by the population of users and thestrength of their response to perturbations, the pure power law is found to becoextensive with an exponential regime (tasks are performed without too muchdelay) and with a crossover to an asymptotic plateau (some tasks are neverperformed). the characterization of the availability and efficiency of humanson their actions revealed by our study have important consequences tounderstand human decision-making, optimal designs of policies such as forinternet security, with spillovers to collective behaviors, crowds dynamics,and social epidemics.
{fenge}
1011.0458	leverage bubble	leverage is strongly related to liquidity in a market and lack of liquidityis considered a cause and/or consequence of the recent financial crisis. arepurchase agreement is a financial instrument where a security is soldsimultaneously with an agreement to buy it back at a later date. repurchaseagreements (repos) market size is a very important element in calculating theoverall leverage in a financial market. therefore, studying the behavior ofrepos market size can help to understand a process that can contribute to thebirth of a financial crisis. we hypothesize that herding behavior among largeinvestors led to massive over-leveraging through the use of repos, resulting ina bubble (built up over the previous years) and subsequent crash in this marketin early 2008. we use the johansen-ledoit-sornette (jls) model of rationalexpectation bubbles and behavioral finance to study the dynamics of the repomarket that led to the crash. the jls model qualifies a bubble by the presenceof characteristic patterns in the price dynamics, called log-periodic power law(lppl) behavior. we show that there was significant lppl behavior in the marketbefore that crash and that the predicted range of times predicted by the modelfor the end of the bubble is consistent with the observations.
{fenge}
1011.4781	icequakes coupled with surface displacements for predicting glacier  break-off	a hanging glacier at the east face of weisshorn (switzerland) broke off in2005. we were able to monitor and measure surface motion and icequake activityfor 25 days up to three days prior to the break-off. the analysis of seismicwaves generated by the glacier during the rupture maturation process revealedfour types of precursory signals of the imminent catastrophic rupture: (i) anincrease in seismic activity within the glacier, (ii) a decrease in the waitingtime between two successive icequakes, (iii) a change in the size-frequencydistribution of icequake energy, and (iv) a modification in the structure ofthe waiting time distributions between two successive icequakes. morevover, itwas possible to demonstrate the existence of a correlation between the seismicactivity and the log-periodic oscillations of the surface velocitiessuperimposed on the global acceleration of the glacier during the rupturematuration. analysis of the seismic activity led us to the identification oftwo regimes: a stable phase with diffuse damage, and an unstable and dangerousphase characterized by a hierarchical cascade of rupture instabilities wherelarge icequakes are triggered.
{fenge}
1011.5343	inferring fundamental value and crash nonlinearity from bubble  calibration	identifying unambiguously the presence of a bubble in an asset price remainsan unsolved problem in standard econometric and financial economic approaches.a large part of the problem is that the fundamental value of an asset is, ingeneral, not directly observable and it is poorly constrained to calculate.further, it is not possible to distinguish between an exponentially growingfundamental price and an exponentially growing bubble price. we present aseries of new models based on the johansen-ledoit-sornette (jls) model, whichis a flexible tool to detect bubbles and predict changes of regime in financialmarkets. our new models identify the fundamental value of an asset price andcrash nonlinearity from a bubble calibration. in addition to forecasting thetime of the end of a bubble, the new models can also estimate the fundamentalvalue and the crash nonlinearity. besides, the crash nonlinearity obtained inthe new models presents a new approach to possibly identify the dynamics of acrash after a bubble. we test the models using data from three historicalbubbles ending in crashes from different markets. they are: the hong kong hangseng index 1997 crash, the s&amp;p 500 index 1987 crash and the shanghai compositeindex 2009 crash. all results suggest that the new models perform very well indescribing bubbles, forecasting their ending times and estimating fundamentalvalue and the crash nonlinearity. the performance of the new models is testedunder both the gaussian and non-gaussian residual assumption. under thegaussian residual assumption, nested hypotheses with the wilks statistics areused and the p-values suggest that models with more parameters are necessary.under non-gaussian residual assumption, we use a bootstrap method to get type iand ii errors of the hypotheses. all tests confirm that the generalized jlsmodels provide useful improvements over the standard jls model.
{fenge}
1011.6472	super-extreme event's influence on a weierstrass-mandelbrot  continuous-time random walk	two utmost cases of super-extreme event's influence on the velocityautocorrelation function (vaf) were considered. the vaf itself was derivedwithin the hierarchical weierstrass-mandelbrot continuous-time random walk(wm-ctrw) formalism, which is able to cover a broad spectrum of continuous-timerandom walks. firstly, we studied a super-extreme event in a form of asustained drift, whose duration time is much longer than that of any otherevent. secondly, we considered a super-extreme event in the form of a shockwith the size and velocity much larger than those corresponding to any otherevent. we found that the appearance of these super-extreme events substantiallychanges the results determined by extreme events (the so called "black swans")that are endogenous to the wm-ctrw process. for example, changes of the vaf inthe latter case are in the form of some instability and distinctly differ fromthose caused in the former case. in each case these changes are quite differentcompared to the situation without super-extreme events suggesting thepossibility to detect them in natural system if they occur.
{fenge}
1101.2832	evidence for super-exponentially accelerating atmospheric carbon dioxide  growth	we analyze the growth rates of atmospheric carbon dioxide and humanpopulation, by comparing the relative merits of two benchmark models, theexponential law and the finite-time-singular (fts) power law. the later resultsfrom positive feedbacks, either direct or mediated by other dynamicalvariables, as shown in our presentation of a simple endogenous macroeconomicdynamical growth model. our empirical calibrations finds that the humanpopulation has decelerated from its previous super-exponential growth until1960 to a slower-than-exponential growth associated with a decreasing growthrate. however, the past decade is found to be characterized by an almost stablegrowth rate approximately equal to r(2010) ~ 1% per year, suggesting that thepopulation growth is stabilizing at "just" an exponential growth. as foratmospheric co2 content, we find that it is at least exponentially increasingand most likely characterized by an accelerating growth rate as off 2009,consistent with an unsustainable fts power law regime announcing a drasticchange of regime. the coexistence of a quasi-exponential growth of humanpopulation with a super-exponential growth of carbon dioxide content in theatmosphere is a diagnostic that, until now, improvements in carbon efficiencyper unit of production worldwide has been dramatically insufficient.
{fenge}
1101.5062	climate warming and stability of cold hanging glaciers: lessons from the  gigantic 1895 altels break-off	the altels hanging glacier broke off on september 11, 1895. the ice volume ofthis catastrophic rupture was estimated at $\rm 4.10^6$ cubic meters and is thelargest ever observed ice fall event in the alps. the causes of this collapseare however not entirely clear. based on previous studies, we reanalyzed thisbreak-off event, with the help of a new numerical model, initially developed byfaillettaz and others (2010) for gravity-driven instabilities. the simulationsindicate that a break-off event is only possible when the basal friction at thebedrock is reduced in a restricted area, possibly induced by the storage ofinfiltrated water within the glacier. moreover, our simulations reveal atwo-step behavior: (i) a first quiescent phase, without visible changes, with aduration depending on the rate of basal changes; (ii) an active phase with arapid increase of basal motion over a few days. the general lesson obtainedfrom the comparison between the simulations and the available evidence is thatvisible signs of the destabilization process of a hanging glacier, resultingfrom a progressive warming of the ice/bed interface towards a temperate regime,will appear just a few days prior to the collapse.
{fenge}
1101.5888	predicted and verified deviations from zipf's law in ecology of  competing products	zipf's power-law distribution is a generic empirical statistical regularityfound in many complex systems. however, rather than universality with a singlepower-law exponent (equal to 1 for zipf's law), there are many reporteddeviations that remain unexplained. a recently developed theory finds that theinterplay between (i) one of the most universal ingredients, namely stochasticproportional growth, and (ii) birth and death processes, leads to a genericpower-law distribution with an exponent that depends on the characteristics ofeach ingredient. here, we report the first complete empirical test of thetheory and its application, based on the empirical analysis of the dynamics ofmarket shares in the product market. we estimate directly the average growthrate of market shares and its standard deviation, the birth rates and the"death" (hazard) rate of products. we find that temporal variations and productdifferences of the observed power-law exponents can be fully captured by thetheory with no adjustable parameters. our results can be generalized to manysystems for which the statistical properties revealed by power law exponentsare directly linked to the underlying generating mechanism.
{fenge}
1102.2138	the us stock market leads the federal funds rate and treasury bond  yields	using a recently introduced method to quantify the time varying lead-lagdependencies between pairs of economic time series (the thermal optimal pathmethod), we test two fundamental tenets of the theory of fixed income: (i) thestock market variations and the yield changes should be anti-correlated; (ii)the change in central bank rates, as a proxy of the monetary policy of thecentral bank, should be a predictor of the future stock market direction. usingboth monthly and weekly data, we found very similar lead-lag dependence betweenthe s&amp;p500 stock market index and the yields of bonds inside two groups: bondyields of short-term maturities (federal funds rate (ffr), 3m, 6m, 1y, 2y, and3y) and bond yields of long-term maturities (5y, 7y, 10y, and 20y). in allcases, we observe the opposite of (i) and (ii). first, the stock market andyields move in the same direction. second, the stock market leads the yields,including and especially the ffr. moreover, we find that the short-term yieldsin the first group lead the long-term yields in the second group before thefinancial crisis that started mid-2007 and the inverse relationship holdsafterwards. these results suggest that the federal reserve is increasinglymindful of the stock market behavior, seen at key to the recovery and health ofthe economy. long-term investors seem also to have been more reactive andmindful of the signals provided by the financial stock markets than the federalreserve itself after the start of the financial crisis. the lead of the s&amp;p500stock market index over the bond yields of all maturities is confirmed by thetraditional lagged cross-correlation analysis.
{fenge}
1104.3616	strategies used as spectroscopy of financial markets reveal new stylized  facts	we propose a new set of stylized facts quantifying the structure of financialmarkets. the key idea is to study the combined structure of both investmentstrategies and prices in order to open a qualitatively new level ofunderstanding of financial and economic markets. we study the detailed orderflow on the shenzhen stock exchange of china for the whole year of 2003. thisenormous dataset allows us to compare (i) a closed national market (a-shares)with an international market (b-shares), (ii) individuals and institutions and(iii) real investors to random strategies with respect to timing that shareotherwise all other characteristics. we find that more trading results insmaller net return due to trading frictions. we unveiled quantitative powerlaws with non-trivial exponents, that quantify the deterioration of performancewith frequency and with holding period of the strategies used by investors.random strategies are found to perform much better than real ones, both forwinners and losers. surprising large arbitrage opportunities exist, especiallywhen using zero-intelligence strategies. this is a diagnostic of possibleinefficiencies of these financial markets.
{fenge}
0710.0317	a general strategy for physics-based model validation illustrated with  earthquake phenomenology, atmospheric radiative transfer, and computational  fluid dynamics	validation is often defined as the process of determining the degree to whicha model is an accurate representation of the real world from the perspective ofits intended uses. validation is crucial as industries and governments dependincreasingly on predictions by computer models to justify their decisions. inthis article, we survey the model validation literature and propose toformulate validation as an iterative construction process that mimics theprocess occurring implicitly in the minds of scientists. we thus offer a formalrepresentation of the progressive build-up of trust in the model, and therebyreplace incapacitating claims on the impossibility of validating a given modelby an adaptive process of constructive approximation. this approach is betteradapted to the fuzzy, coarse-grained nature of validation. our procedurefactors in the degree of redundancy versus novelty of the experiments used forvalidation as well as the degree to which the model predicts the observations.we illustrate the new methodology first with the maturation of quantummechanics as the arguably best established physics theory and then with severalconcrete examples drawn from some of our primary scientific interests: acellular automaton model for earthquakes, an anomalous diffusion model forsolar radiation transport in the cloudy atmosphere, and a computational fluiddynamics code for the richtmyer-meshkov instability. this article is anaugmented version of sornette et al. [2007] that appeared in proceedings of thenational academy of sciences in 2007 (doi: 10.1073/pnas.0611677104), with anelectronic supplement at urlhttp://www.pnas.org/cgi/content/full/0611677104/dc1. sornette et al. [2007] isalso available in preprint form at physics/0511219.
{fenge}
1107.0838	role of diversification risk in financial bubbles	we present an extension of the johansen-ledoit-sornette (jls) model toinclude an additional pricing factor called the "zipf factor", which describesthe diversification risk of the stock market portfolio. keeping all thedynamical characteristics of a bubble described in the jls model, the new modelprovides additional information about the concentration of stock gains overtime. this allows us to understand better the risk diversification and toexplain the investors' behavior during the bubble generation. we apply this newmodel to two famous chinese stock bubbles, from august 2006 to october 2007(bubble 1) and from october 2008 to august 2009 (bubble 2). the zipf factor isfound highly significant for bubble 1, corresponding to the fact that valuationgains were more concentrated on the large firms of the shanghai index. it islikely that the widespread acknowledgement of the 80-20 rule in the chinesemedia and discussion forums led many investors to discount the risk of a lackof diversification, therefore enhancing the role of the zipf factor. for bubble2, the zipf factor is found marginally relevant, suggesting a larger weight ofmarket gains on small firms. we interpret this result as the consequence of theresponse of the chinese economy to the very large stimulus provided by thechinese government in the aftermath of the 2008 financial crisis.
{fenge}
1108.0099	a stable and robust calibration scheme of the log-periodic power law  model	we present a simple transformation of the formulation of the log-periodicpower law formula of the johansen-ledoit-sornette model of financial bubblesthat reduces it to a function of only three nonlinear parameters. thetransformation significantly decreases the complexity of the fitting procedureand improves its stability tremendously because the modified cost function isnow characterized by good smooth properties with in general a single minimum inthe case where the model is appropriate to the empirical data. we complementthe approach with an additional subordination procedure that slaves two of thenonlinear parameters to what can be considered to be the most crucial nonlinearparameter, the critical time $t_c$ defined as the end of the bubble and themost probably time for a crash to occur. this further decreases the complexityof the search and provides an intuitive representation of the results of thecalibration. with our proposed methodology, metaheuristic searches are notlonger necessary and one can resort solely to rigorous controlled local searchalgorithms, leading to dramatic increase in efficiency. empirical tests on theshanghai composite index (sse) from january 2007 to march 2008 illustrate ourfindings.
{fenge}
1112.6024	valuation of zynga	on december 16, zynga, the well-known social game developing company wentpublic. this event is following other recent ipos in the world of socialnetworking companies, such as groupon, linkedin or pandora to cite a few. witha valuation close to 7 billion usd at the time when it went public, zynga hasbecome the biggest web ipo since google. this recent enthusiasm for socialnetworking companies, and in particular zynga, brings up the question whetheror not they are overvalued. the common denominator of all these ipos is that alot of estimates about their valuation have been circulating, without anyspecifics given about the methodology or assumptions used to obtain thosenumbers. to bring more substance to the debate, we propose a two-tieredapproach. first, we introduce a new model to forecast the global user base ofzynga, based on the analysis of the individual dynamics of its major games.next, we model the revenues per user using a logistic growth function, astandard model for growth in competition. this leads to bracket the valuationof zynga using three different scenarios (base one, optimistic and veryoptimistic): 4.17 billion usd in the base case, 5.16 billion in the high growthand 7.02 billion in the extreme growth scenario respectively. thus, only theunlikely extreme growth scenario could potentially justify today's 6.6 billionusd valuation of zynga. this suggests that zynga at its ipo has beenoverpriced.
{fenge}
1201.1189	prediction of alpine glacier sliding instabilities: a new hope	mechanical and sliding instabilities are the two processes which may lead tobreaking off events of large ice masses. mechanical instabilities mainly affectunbalanced cold hanging glaciers. for the latter case, a prediction could beachieved based on data of surface velocities and seismic activity. the case ofsliding instabilities is more problematic. this phenomenon occurs on temperateglacier tongues. such instabilities are strongly affected by the subglacialhydrology: melt water may cause (i) a lubrication of the bed and (ii) adecrease of the effective pressure and consequently a decrease of basalfriction. available data from allalingletscher (valais) indicate that theglacier tongue experienced an active phase during 2-3 weeks with enhanced basalmotion in late summer in most years. in order to scrutinize in more detail theprocesses governing the sliding instabilities, a numerical model developed toinvestigate gravitational instabilities in heterogeneous media was applied toallalingletscher. this model enables to account for various geometricconfigurations, interaction between sliding and tension cracking and water flowat the bedrock. we could show that both a critical geometrical configuration ofthe glacier tongue and the existence of a distributed drainage network were themain causes of this catastrophic break-off. moreover, the analysis of themodeling results diagnose the phenomenon of recoupling of the glacier to itsbed as a potential new precursory sign announcing the final break-off. thismodel casts a gleam of hope for a better understanding of the ultimate ruptureof such glacier sliding instabilities.
{fenge}
1202.3936	on the distribution of time-to-proof of mathematical conjectures	what is the productivity of science? can we measure an evolution of theproduction of mathematicians over history? can we predict the waiting time tillthe proof of a challenging conjecture such as the p-versus-np problem?motivated by these questions, we revisit a suggestion published recently anddebated in the "new scientist" that the historical distribution oftime-to-proof's, i.e., of waiting times between formulation of a mathematicalconjecture and its proof, can be quantified and gives meaningful insights inthe future development of still open conjectures. we find however evidence thatthe mathematical process of creation is too much non-stationary, with toolittle data and constraints, to allow for a meaningful conclusion. inparticular, the approximate unsteady exponential growth of human population,and arguably that of mathematicians, essentially hides the true distribution.another issue is the incompleteness of the dataset available. in conclusion wecannot really reject the simplest model of an exponential rate of conjectureproof with a rate of 0.01/year for the dataset that we have studied,translating into an average waiting time to proof of 100 years. we hope thatthe presented methodology, combining the mathematics of recurrent processes,linking proved and still open conjectures, with different empiricalconstraints, will be useful for other similar investigations probing theproductivity associated with mankind growth and creativity.
{fenge}
1203.6231	strong gender differences in reproductive success variance, and the  times to the most recent common ancestors	the time to the most recent common ancestor (tmrca) based on humanmitochondrial dna (mtdna) is estimated to be twice that based on thenon-recombining part of the y chromosome (nry). these tmrcas have specialdemographic implications because mtdna is transmitted only from mother tochild, and nry from father to son. therefore, mtdna reflects female history,and nry, male history. to investigate what caused the two-to-one female-maletmrca ratio in humans, we develop a forward-looking agent-based model (abm)with overlapping generations and individual life cycles. we implement two mainmating systems: polygynandry and polygyny with different degrees in between. ineach mating system, the male population can be either homogeneous orheterogeneous. in the latter case, some males are `alphas' and others are`betas', which reflects the extent to which they are favored by female mates. aheterogeneous male population implies a competition among males with thepurpose of signaling as alphas. the introduction of a heterogeneous malepopulation is found to reduce by a factor 2 the probability of finding equalfemale and male tmrcas and shifts the distribution of the tmrca ratio to highervalues. we find that high male-male competition is necessary to reproduce atmrca ratio of 2: less than half the males can be alphas and betas can have atmost half the fitness of alphas. in addition, in the modes that maximize theprobability of having a tmrca ratio between 1.5 and 2.5, the present generationhas 1.4 times as many female as male ancestors. we also tested the effect ofsex-biased migration and sex-specific death rates and found that these areunlikely to explain alone the sex-biased tmrca ratio observed in humans. ourresults support the view that we are descended from males who were successfulin a highly competitive context, while females were facing a much smallerfemale-female competition.
{fenge}
1204.0350	when games meet reality: is zynga overvalued?	on december 16th, 2011, zynga, the well-known social game developing companywent public. this event followed other recent ipos in the world of socialnetworking companies, such as groupon or linkedin among others. with avaluation close to 7 billion usd at the time when it went public, zynga becameone of the biggest web ipos since google. this recent enthusiasm for socialnetworking companies raises the question whether they are overvalued. indeed,during the few months since its ipo, zynga showed significant variability, itsmarket capitalization going from 5.6 to 10.2 billion usd, hinting at a possibleirrational behavior from the market. to bring substance to the debate, wepropose a two-tiered approach to compute the intrinsic value of zynga. first,we introduce a new model to forecast its user base, based on the individualdynamics of its major games. next, we model the revenues per user using alogistic function, a standard model for growth in competition. this allows usto bracket the valuation of zynga using three different scenarios: 3.4, 4.0 and4.8 billion usd in the base case, high growth and extreme growth scenariorespectively. this suggests that zynga has been overpriced ever since its ipo.finally, we propose an investment strategy (dated april 19th, 2012 on thearxive), which is based on our diagnostic of a bubble for zynga and how thisherding / bubbly sentiment can be expected to play together with two importantcoming events (the quarterly financial result announcement around april 26th,2012 followed by the end of a first lock-up period around april 30th, 2012). onthe long term, our analysis indicates that zynga's price should decreasesignificantly. the paper ends with a post-mortem analysis added on may 24th,2012, just before going to press, showing that we have successfully predictedthe downward trend of zynga. since april 27th, 2012, zynga dropped 25%.
{fenge}
1205.2915	universality class of balanced flows with bottlenecks: granular flows,  pedestrian fluxes and financial price dynamics	we propose and document the evidence for an analogy between the dynamics ofgranular counter-flows in the presence of bottlenecks or restrictions andfinancial price formation processes. using extensive simulations, we find thatthe counter-flows of simulated pedestrians through a door display many stylizedfacts observed in financial markets when the density around the door iscompared with the logarithm of the price. the stylized properties are presentalready when the agents in the pedestrian model are assumed to display azero-intelligent behavior. if agents are given decision-making capacity andadapt to partially follow the majority, periods of herding behavior mayadditionally occur. this generates the very slow decay of the autocorrelationof absolute return due to an intermittent dynamics. our finding suggest thatthe stylized facts in the fluctuations of the financial prices result from acompetition of two groups with opposite interests in the presence of aconstraint funneling the flow of transactions to a narrow band of prices.
{fenge}
1210.6321	high quality topic extraction from business news explains abnormal  financial market volatility	understanding the mutual relationships between information flows and socialactivity in society today is one of the cornerstones of the social sciences. infinancial economics, the key issue in this regard is understanding andquantifying how news of all possible types (geopolitical, environmental,social, financial, economic, etc.) affect trading and the pricing of firms inorganized stock markets. in this article, we seek to address this issue byperforming an analysis of more than 24 million news records provided bythompson reuters and of their relationship with trading activity for 206 majorstocks in the s&amp;p us stock index. we show that the whole landscape of news thataffect stock price movements can be automatically summarized via simpleregularized regressions between trading activity and news information piecesdecomposed, with the help of simple topic modeling techniques, into their"thematic" features. using these methods, we are able to estimate and quantifythe impacts of news on trading. we introduce network-based visualizationtechniques to represent the whole landscape of news information associated witha basket of stocks. the examination of the words that are representative of thetopic distributions confirms that our method is able to extract the significantpieces of information influencing the stock market. our results show that oneof the most puzzling stylized fact in financial economies, namely that atcertain times trading volumes appear to be "abnormally large," can be partiallyexplained by the flow of news. in this sense, our results prove that there isno "excess trading," when restricting to times when news are genuinely noveland provide relevant financial information.
{fenge}
1211.1949	dynamical diagnosis and solutions for resilient natural and social  systems	the concept of resilience embodies the quest towards the ability to sustainshocks, to suffer from these shocks as little as possible, for the shortesttime possible, and to recover with the full functionalities that existed beforethe perturbation. we propose an operation definition of resilience, seeing itas a measure of stress that is complementary to the risk measures. emphasis isput on the distinction between stressors (the forces acting on the system) andstress (the internal reaction of the system to the stressors). this allows usto elaborate a classification of stress measures and of the possible responsesto stressors. we emphasize the need for characterizing the goals of a givensystem, from which the process of resilience build-up can be defined.distinguishing between exogenous versus endogenous sources of stress allows oneto define the corresponding appropriate responses. the main ingredients towardsresilience include (1) the need for continuous multi-variable measurement anddiagnosis of endogenous instabilities, (2) diversification and heterogeneity,(3) decoupling, (4) incentives and motivations, and (5) last but not least the(obvious) role of individual strengths. propositions for individual trainingtowards resilience are articulated. the concept of "crisis flight simulators"is introduced to address the intrinsic human cognitive biases underlying thelogic of failures and the illusion of control. we also introduce the"time-at-risk" framework, whose goal is to provide continuous predictiveupdates on possible scenarios and their probabilistic weights, so that aculture of preparedness and adaptation be promoted. these concepts arepresented towards building up personal resilience, resilient societies andresilient financial systems.
{fenge}
1304.6912	automatic reconstruction of fault networks from seismicity catalogs  including location uncertainty	we introduce the anisotropic clustering of location uncertainty distributions(aclud) method to reconstruct active fault networks on the basis of bothearthquake locations and their estimated individual uncertainties. after amassive search through the large solution space of possible reconstructed faultnetworks, we apply six different validation procedures in order to select thecorresponding best fault network. two of the validation steps (cross-validationand bayesian information criterion (bic) process the fit residuals, while thefour others look for solutions that provide the best agreement withindependently observed focal mechanisms. tests on synthetic catalogs allow usto qualify the performance of the fitting method and of the various validationprocedures. the aclud method is able to provide solutions that are close to theexpected ones, especially for the bic and focal mechanismbased techniques. theclustering method complemented by the validation step based on focal mechanismsprovides good solutions even in the presence of a significant spatialbackground seismicity rate. our new fault reconstruction method is then appliedto the landers area in southern california and compared with previousclustering methods. the results stress the importance of taking into accountundersampled sub-fault structures as well as of the spatially inhomogeneouslocation uncertainties.
{fenge}
1308.6756	apparent criticality and calibration issues in the hawkes self-excited  point process model: application to high-frequency financial data	we present a careful analysis of possible issues on the application of theself-excited hawkes process to high-frequency financial data. we carefullyanalyze a set of effects leading to significant biases in the estimation of the"criticality index" n that quantifies the degree of endogeneity of how muchpast events trigger future events. we report a number of model biases that areintrinsic to the estimation of brnaching ratio (n) when using power law memorykernels. we demonstrate that the calibration of the hawkes process on mixturesof pure poisson process with changes of regime leads to completely spuriousapparent critical values for the branching ratio (n~1) while the true value isactually n=0. more generally, regime shifts on the parameters of the hawkesmodel and/or on the generating process itself are shown to systematically leadto a significant upward bias in the estimation of the branching ratio. we alsodemonstrate the importance of the preparation of the high-frequency financialdata and give special care to the decrease of quality of the timestamps of tickdata due to latency and grouping of messages to packets by the stock exchange.altogether, our careful exploration of the caveats of the calibration of thehawkes process stresses the need for considering all the above issues beforeany conclusion can be sustained. in this respect, because the above effects areplaguing their analyses, the claim by hardiman, bercot and bouchaud (2013) thatfinancial market have been continuously functioning at or close to criticality(n~1) cannot be supported. in contrast, our previous results on e-mini s&amp;p 500futures contracts and on major commodity future contracts are upheld.
{fenge}
0802.0074	novel insights into the dynamics of intractable human epilepsy	probability density functions and the probability of sz occurrenceconditional upon the time elapsed from the previous sz were estimated using theenergy and intervals of sz in prolonged recordings from subjects withlocalization- related pharmaco-resistant epilepsy, undergoing surgicalevaluation. clinical and subclinical seizure e and isi distributions aregoverned by power laws in subjects on reduced doses of anti-seizure drugs.there is increased probability of sz occurrence 30 minutes before and after aseizure and the time to next seizure increases with the duration of theseizure-free interval since the last one. also, over short time scales,``seizures may beget seizures.'' the cumulative empirical evidence iscompatible with and suggests that at least over short time scales, seizureshave the inherent capacity of triggering other seizures. this may explain thetendency of seizures to cluster and evolve into status epilepticus. power lawdistributions of e and isi indicate these features lack a typical size/durationand may not be accurate criteria or sufficient for classifying paroxysmalactivity as ictal or interictal. this dependency and the existence of power lawdistributions raise the possibility that sz occurrence and intensity may bepredictable, without specifying the likelihood of success.
{fenge}
1312.4644	a generic model of dyadic social relationships	we introduce a model of dyadic social interactions and establish itscorrespondence with relational models theory (rmt), a theory of human socialrelationships. rmt posits four elementary models of relationships governinghuman interactions, singly or in combination: communal sharing, authorityranking, equality matching, and market pricing. to these are added the limitingcases of asocial and null interactions, whereby people do not coordinate withreference to any shared principle. our model is rooted in the observation thateach individual in a dyadic interaction can do either the same thing as theother individual, a different thing or nothing at all. to represent these threepossibilities, we consider two individuals that can each act in one out ofthree ways toward the other: perform a social action x or y, or alternativelydo nothing. we demonstrate that the relationships generated by this modelaggregate into six exhaustive and disjoint categories. we propose that four ofthese categories match the four relational models, while the remaining twocorrespond to the asocial and null interactions defined in rmt. we generalizeour results to the presence of n social actions. we infer that the fourrelational models form an exhaustive set of all possible dyadic relationshipsbased on social coordination. hence, we contribute to rmt by offering an answerto the question of why there could exist just four relational models. inaddition, we discuss how to use our representation to analyze data sets ofdyadic social interactions, and how social actions may be valued and matched bythe agents.
{fenge}
0704.0589	analysis of the real estate market in las vegas: bubble, seasonal  patterns, and prediction of the csw indexes	we analyze 27 house price indexes of las vegas from jun. 1983 to mar. 2005,corresponding to 27 different zip codes. these analyses confirm the existenceof a real-estate bubble, defined as a price acceleration faster thanexponential, which is found however to be confined to a rather limited timeinterval in the recent past from approximately 2003 to mid-2004 and hasprogressively transformed into a more normal growth rate comparable topre-bubble levels in 2005. there has been no bubble till 2002 except for amedium-sized surge in 1990. in addition, we have identified a strong yearlyperiodicity which provides a good potential for fine-tuned prediction frommonth to month. a monthly monitoring using a model that we have developed couldconfirm, by testing the intra-year structure, if indeed the market has returnedto ``normal'' or if more turbulence is expected ahead. we predict the evolutionof the indexes one year ahead, which is validated with new data up to sep.2006. the present analysis demonstrates the existence of very significantvariations at the local scale, in the sense that the bubble in las vegas seemsto have preceded the more global usa bubble and has ended approximately twoyears earlier (mid 2004 for las vegas compared with mid-2006 for the whole ofthe usa).
{fenge}
1403.3228	fractal multi-level organisation of human groups in a virtual world	humans are fundamentally social. they have progressively dominated theirenvironment by the strength and creativity provided by and within theirgrouping. it is well recognised that human groups are highly structured, andthe anthropological literature has loosely classified them according to theirsize and function, such as support cliques, sympathy groups, bands, cognitivegroups, tribes, linguistic groups and so on. recently, combining data on humangrouping patterns in a comprehensive and systematic study, zhou et al.identified a quantitative discrete hierarchy of group sizes with a preferredscaling ratio close to $3$, which was later confirmed for hunter-gatherergroups and for other mammalian societies. using high precision large scaleinternet-based social network data, we extend these early findings on a verylarge data set. we analyse the organisational structure of a complete,multi-relational, large social multiplex network of a human society consistingof about 400,000 odd players of a massive multiplayer online game for which weknow all about the group memberships of every player. remarkably, the onlineplayers exhibit the same type of structured hierarchical layers as thesocieties studied by anthropologists, where each of these layers is three tofour times the size of the lower layer. our findings suggest that thehierarchical organisation of human society is deeply nested in humanpsychology.
{fenge}
1404.2140	financial bubbles: mechanisms and diagnostics	we define a financial bubble as a period of unsustainable growth, when theprice of an asset increases ever more quickly, in a series of acceleratingphases of corrections and rebounds. more technically, during a bubble phase,the price follows a faster-than-exponential power law growth process, oftenaccompanied by log-periodic oscillations. this dynamic ends abruptly in achange of regime that may be a crash or a substantial correction. because theyleave such specific traces, bubbles may be recognised in advance, that is,before they burst. in this paper, we will explain the mechanism behindfinancial bubbles in an intuitive way. we will show how the log-periodic powerlaw emerges spontaneously from the complex system that financial markets are,as a consequence of feedback mechanisms, hierarchical structure and specifictrading dynamics and investment styles. we argue that the risk of a majorcorrection, or even a crash, becomes substantial when a bubble develops towardsmaturity, and that it is therefore very important to find evidence of bubblesand to follow their development from as early a stage as possible. the toolsthat are explained in this paper actually serve that purpose. they are at thecore of the financial crisis observatory at the eth zurich, where tens ofthousands of assets are monitored on a daily basis. this allow us to have acontinuous overview of emerging bubbles in the global financial markets. thecompanion report available as part of the notenstein white paper series (2014)with the title ``financial bubbles: mechanism, diagnostic and state of theworld (feb. 2014)'' presents a practical application of the methodologyoutlines in this article and describes our view of the status concerningpositive and negative bubbles in the financial markets, as of the end ofjanuary 2014.
{fenge}
1405.4298	how much is the whole really more than the sum of its parts? 1 + 1 =  2.5: superlinear productivity in collective group actions	in a variety of open source software projects, we document a superlineargrowth of production ($r \sim c^\beta$) as a function of the number of activedevelopers $c$, with $\beta \simeq 4/3$ with large dispersions. for a typicalproject in this class, doubling of the group size multiplies typically theoutput by a factor $2^\beta=2.5$, explaining the title. this superlinear law isfound to hold for group sizes ranging from 5 to a few hundred developers. wepropose two classes of mechanisms, {\it interaction-based} and {\it largedeviation}, along with a cascade model of productive activity, which unifiesthem. in this common framework, superlinear productivity requires that theinvolved social groups function at or close to criticality, in the sense of asubtle balance between order and disorder. we report the first empirical testof the renormalization of the exponent of the distribution of the sizes offirst generation events into the renormalized exponent of the distribution ofclusters resulting from the cascade of triggering over all generation in acritical branching process in the non-meanfield regime. finally, we document asize effect in the strength and variability of the superlinear effect, withsmaller groups exhibiting widely distributed superlinear exponents, some ofthem characterizing highly productive teams. in contrast, large groups tend tohave a smaller superlinearity and less variability.
{fenge}
1407.5037	power law scaling and "dragon-kings" in distributions of intraday  financial drawdowns	we investigate the distributions of epsilon-drawdowns and epsilon-drawups ofthe most liquid futures financial contracts of the world at time scales of 30seconds. the epsilon-drawdowns (resp. epsilon- drawups) generalise the notionof runs of negative (resp. positive) returns so as to capture the risks towhich investors are arguably the most concerned with. similarly to thedistribution of returns, we find that the distributions of epsilon-drawdownsand epsilon-drawups exhibit power law tails, albeit with exponentssignificantly larger than those for the return distributions. this paradoxicalresult can be attributed to (i) the existence of significant transientdependence between returns and (ii) the presence of large outliers(dragon-kings) characterizing the extreme tail of the drawdown/drawupdistributions deviating from the power law. the study of the tail dependencebetween the sizes, speeds and durations of drawdown/drawup indicates a clearrelationship between size and speed but none between size and duration. thisimplies that the most extreme drawdown/drawup tend to occur fast and aredominated by a few very large returns. we discuss both the endogenous andexogenous origins of these extreme events.
{fenge}
1407.7118	estimation of the hawkes process with renewal immigration using the em  algorithm	we introduce the hawkes process with renewal immigration and make itsstatistical estimation possible with two expectation maximization (em)algorithms. the standard hawkes process introduces immigrant points via apoisson process, and each immigrant has a subsequent cluster of associatedoffspring of multiple generations. we generalize the immigration to come from arenewal process; introducing dependence between neighbouring clusters, andallowing for over/under dispersion in cluster locations. this complicatesevaluation of the likelihood since one needs to know which subset of theobserved points are immigrants. two em algorithms enable estimation here: thefirst is an extension of an existing algorithm that treats the entire branchingstructure - which points are immigrants, and which point is the parent of eachoffspring - as missing data. the second considers only if a point is animmigrant or not as missing data and can be implemented with linear timecomplexity. both algorithms are found to be consistent in simulation studies.further, we show that misspecifying the immigration process introducessignficant bias into model estimation-- especially the branching ratio, whichquantifies the strength of self excitation. thus, this extended model providesa valuable alternative model in practice.
{fenge}
1408.5618	symmetric thermal optimal path and time-dependent lead-lag relationship:  novel statistical tests and application to uk and us real-estate and monetary  policies	we present the symmetric thermal optimal path (tops) method to determine thetime-dependent lead-lag relationship between two stochastic time series. thisnovel version of the previously introduced top method alleviates someinconsistencies by imposing that the lead-lag relationship should be invariantwith respect to a time reversal of the time series after a change of sign. thismeans that, if `$x$ comes before $y$', this transforms into `$y$ comes before$x$' under a time reversal. we show that previously proposed bootstrap testlacks power and leads too often to a lack of rejection of the null that thereis no lead-lag correlation when it is present. we introduce instead two noveltests. the first free energy p-value $\rho$ criterion quantifies theprobability that a given lead-lag structure could be obtained from random timeseries with similar characteristics except of the lead-lag information. thesecond self-consistent test embodies the idea that, for the lead-lag path to besignificant, synchronising the two time series using the time varying lead-lagpath should lead to a statistically significant correlation. we performintensive synthetic tests to demonstrate their performance and limitations.finally, we apply the tops method with the two new tests to the time dependentlead-lag structures of house price and monetary policy of the united kingdom(uk) and united states (us) from 1991 to 2011. the tops approach stresses theimportance of accounting for change of regimes, so that similar pieces ofinformation or policies may have drastically different impacts anddevelopments, conditional on the economic, financial and geopoliticalconditions. this study reinforces the view that the hypothesis of statisticalstationarity is highly questionable.
{fenge}
1504.02380	of disasters and dragon kings: a statistical analysis of nuclear power  incidents &amp; accidents	we provide, and perform a risk theoretic statistical analysis of, a datasetthat is 75 percent larger than the previous best dataset on nuclear incidentsand accidents, comparing three measures of severity: ines (internationalnuclear event scale), radiation released, and damage dollar losses. the annualrate of nuclear accidents, with size above 20 million us$, per plant, decreasedfrom the 1950s until dropping significantly after chernobyl (april, 1986). therate is now roughly stable at 0.002 to 0.003, i.e., around 1 event per yearacross the current fleet. the distribution of damage values changed after threemile island (tmi; march, 1979), where moderate damages were suppressed but thetail became very heavy, being described by a pareto distribution with tailindex 0.55. further, there is a runaway disaster regime, associated with the"dragon-king" phenomenon, amplifying the risk of extreme damage. in fact, thedamage of the largest event (fukushima; march, 2011) is equal to 60 percent ofthe total damage of all 174 accidents in our database since 1946. in dollarlosses we compute a 50% chance that (i) a fukushima event (or larger) occurs inthe next 50 years, (ii) a chernobyl event (or larger) occurs in the next 27years and (iii) a tmi event (or larger) occurs in the next 10 years. finally,we find that the ines scale is inconsistent. to be consistent with damage, thefukushima disaster would need to have an ines level of 11, rather than themaximum of 7.
{fenge}
1507.08689	multiple outlier detection in samples with exponential &amp; pareto tails:  redeeming the inward approach &amp; detecting dragon kings	we consider the detection of multiple outliers in exponential and paretosamples -- as well as general samples that have approximately exponential orpareto tails, thanks to extreme value theory. it is shown that a simple"robust" modification of common test statistics makes inward sequential testing-- formerly relegated within the literature since the introduction of outwardtesting -- as powerful as, and potentially less error prone than, outwardtests. moreover, inward testing does not require the complicated type 1 errorcontrol of outward tests. a variety of test statistics, employed in both blockand sequential tests, are compared for their power and errors, in casesincluding no outliers, dispersed outliers (the classical slippage alternative),and clustered outliers (a case seldom considered). we advocate a densitymixture approach for detecting clustered outliers. tests are found to be highlysensitive to the correct specification of the main distribution(exponential/pareto), exposing high potential for errors in inference. further,in five case studies -- financial crashes, nuclear power generation accidents,stock market returns, epidemic fatalities, and cities within countries --significant outliers are detected and related to the concept of "dragon king"events, defined as meaningful outliers of unique origin.
{fenge}
1508.04754	currency target zone modeling: an interplay between physics and  economics	we study the performance of the euro/swiss franc exchange rate in theextraordinary period from september 6, 2011 and january 15, 2015 when the swissnational bank enforced a minimum exchange rate of 1.20 swiss francs per euro.based on the analogy between brownian motion in finance and physics, thefirst-order effect of such a steric constraint would enter a priori in the formof a repulsive entropic force associated with the paths crossing the barrierthat are forbidden. non-parametric empirical estimates of drift and volatilityshow that the predicted first-order analogy between economics and physics areincorrect. the clue is to realise that the random walk nature of financialprices results from the continuous anticipations of traders about futureopportunities, whose aggregate actions translate into an approximate efficientmarket with almost no arbitrage opportunities. with the swiss national bankstated commitment to enforce the barrier, traders's anticipation of this actionleads to a vanishing drift together with a volatility of the exchange rate thatdepends on the distance to the barrier. we give direct quantitative empiricalevidence that this effect is well described by krugman's target zone model[p.r. krugman. the quarterly journal of economics, 106(3):669-682, 1991].motivated by the insights from this economical model, we revise the initialeconomics-physics analogy and show that, within the context of hindereddiffusion, the two systems can be described with the same mathematics afterall. using a recently proposed extended analogy in terms of a colloidalbrownian particle embedded in a fluid of molecules associated with theunderlying order book, we derive that, close to the restricting boundary, thedynamics of both systems is described by a stochastic differential equationwith a very small constant drift and a linear diffusion coefficient.
{fenge}
1508.06024	financial knudsen number: breakdown of continuous price dynamics and  asymmetric buy and sell structures confirmed by high precision order book  information	we generalise the description of the dynamics of the order book of financialmarkets in terms of a brownian particle embedded in a fluid of incoming,exiting and annihilating particles by presenting a model of the velocity oneach side (buy and sell) independently. the improved model builds on thetime-averaged number of particles in the inner layer and its change per unittime, where the inner layer is revealed by the correlations between pricevelocity and change in the number of particles (limit orders). this allows usto introduce the knudsen number of the financial brownian particle motion andits asymmetric version (on the buy and sell sides). not being consideredpreviously, the asymmetric knudsen numbers are crucial in finance in order todetect asymmetric price changes. the knudsen numbers allows us to characterisethe conditions for the market dynamics to be correctly described by acontinuous stochastic process. not questioned until now for large liquidmarkets such as the usd/jpy and eur/usd exchange rates, we show that there areregimes when the knudsen numbers are so high that discrete particle effectsdominate, such as during market stresses and crashes. we document the presenceof imbalances of particles depletion rates on the buy and sell sides that areassociated with high knudsen numbers and violent directional price changes.this indicator can detect the direction of the price motion at the early stagewhile the usual volatility risk measure is blind to the price direction.
{fenge}
1508.07503	two-state markov-chain poisson nature of individual cellphone call  statistics	humans are heterogenous and the behaviors of individuals could be differentfrom that at the population level. we conduct an in-depth study of the temporalpatterns of cellphone conversation activities of 73'339 anonymous cellphoneusers with the same truncated weibull distribution of inter-call durations. wefind that the individual call events exhibit a pattern of bursts, in which highactivity periods are alternated with low activity periods. surprisingly, thenumber of events in high activity periods are found to conform to a power-lawdistribution at the population level, but follow an exponential distribution atthe individual level, which is a hallmark of absence of memory in individualcall activities. such exponential distribution is also observed for the numberof events in low activity periods. together with the exponential distributionsof inter-call durations within bursts and of the intervals between consecutivebursts, we demonstrate that the individual call activities are driven by twoindependent poisson processes, which can be combined within a minimal model interms of a two-state first-order markov chain giving very good agreement withthe empirical distributions using the parameters estimated from real data forabout half of the individuals in our sample. by measuring directly thedistributions of call rates across the population, which exhibit power-lawtails, we explain the difference with previous population level studies,purporting the existence of power-law distributions, via the "superposition ofdistributions" mechanism: the superposition of many exponential distributionsof activities with a power-law distribution of their characteristic scalesleads to a power-law distribution of the activities at the population level.
{fenge}
1510.08162	"speculative influence network" during financial bubbles: application to  chinese stock markets	we introduce the speculative influence network (sin) to decipher the causalrelationships between sectors (and/or firms) during financial bubbles. the sinis constructed in two steps. first, we develop a hidden markov model (hmm) ofregime-switching between a normal market phase represented by a geometricbrownian motion (gbm) and a bubble regime represented by the stochasticsuper-exponential sornette-andersen (2002) bubble model. the calibration of thehmm provides the probability at each time for a given security to be in thebubble regime. conditional on two assets being qualified in the bubble regime,we then use the transfer entropy to quantify the influence of the returns ofone asset $i$ onto another asset $j$, from which we introduce the adjacencymatrix of the sin among securities. we apply our technology to the chinesestock market during the period 2005-2008, during which a normal phase wasfollowed by a spectacular bubble ending in a massive correction. we introducethe net speculative influence intensity (nsii) variable as the differencebetween the transfer entropies from $i$ to $j$ and from $j$ to $i$, which isused in a series of rank ordered regressions to predict the maximum loss(\%{maxloss}) endured during the crash. the sectors that influenced othersectors the most are found to have the largest losses. there is a clearprediction skill obtained by using the transfer entropy involving industrialsectors to explain the \%{maxloss} of financial institutions but not viceversa. we also show that the bubble state variable calibrated on the chinesemarket data corresponds well to the regimes when the market exhibits a strongprice acceleration followed by clear change of price regimes. our resultssuggest that sin may contribute significant skill to the development of generallinkage-based systemic risks measures and early warning metrics.
{fenge}
1512.03618	macroeconomic dynamics of assets, leverage and trust	a macroeconomic model based on the economic variables (i) assets, (ii)leverage (defined as debt over asset) and (iii) trust (defined as the maximumsustainable leverage) is proposed to investigate the role of credit in thedynamics of economic growth, and how credit may be associated with botheconomic performance and confidence. our first notable finding is the mechanismof reward/penalty associated with patience, as quantified by the return onassets. in regular economies where the ebita/assets ratio is larger than thecost of debt, starting with a trust higher than leverage results in the highestlong-term return on assets (which can be seen as a proxy for economic growth).our second main finding concerns a recommendation for the reaction of a centralbank to an external shock that affects negatively the economic growth. we findthat late policy intervention in the model economy results in the highestlong-term return on assets and largest asset value. but this comes at the costof suffering longer from the crisis until the intervention occurs. thephenomenon can be ascribed to the fact that postponing intervention allowstrust to increase first, and it is most effective to intervene when trust ishigh. these results derive from two fundamental assumptions underlying ourmodel: (a) trust tends to increase when it is above leverage; (b) economicagents learn optimally to adjust debt for a given level of trust and amount ofassets. using a markov switching model for the ebita/assets ratio, we havesuccessfully calibrated our model to the empirical data of the return on equityof the euro stoxx 50 for the time period 2000-2013. we find that dynamics ofleverage and trust can be highly non-monotonous with curved trajectories, as aresult of the nonlinear coupling between the variables.
{fenge}
0806.2989	how to grow a bubble: a model of myopic adapting agents	we present a simple agent-based model to study the development of a bubbleand the consequential crash and investigate how their proximate triggeringfactor might relate to their fundamental mechanism, and vice versa. our agentsinvest according to their opinion on future price movements, which is based onthree sources of information, (i) public information, i.e. news, (ii)information from their "friendship" network and (iii) private information. ourbounded rational agents continuously adapt their trading strategy to thecurrent market regime by weighting each of these sources of information intheir trading decision according to its recent predicting performance. we findthat bubbles originate from a random lucky streak of positive news, which, dueto a feedback mechanism of these news on the agents' strategies develop into atransient collective herding regime. after this self-amplified exuberance, theprice has reached an unsustainable high value, being corrected by a crash,which brings the price even below its fundamental value. these ingredientsprovide a simple mechanism for the excess volatility documented in financialmarkets. paradoxically, it is the attempt for investors to adapt to the currentmarket regime which leads to a dramatic amplification of the price volatility.a positive feedback loop is created by the two dominating mechanisms(adaptation and imitation) which, by reinforcing each other, result in bubblesand crashes. the model offers a simple reconciliation of the two opposite(herding versus fundamental) proposals for the origin of crashes within asingle framework and justifies the existence of two populations in thedistribution of returns, exemplifying the concept that crashes arequalitatively different from the rest of the price moves.
{fenge}
1607.04136	secular bipolar growth rate of the real us gdp per capita: implications  for understanding past and future economic growth	we present a quantitative characterisation of the fluctuations of theannualized growth rate of the real us gdp per capita growth at many scales,using a wavelet transform analysis of two data sets, quarterly data from 1947to 2015 and annual data from 1800 to 2010. our main finding is that thedistribution of gdp growth rates can be well approximated by a bimodal functionassociated to a series of switches between regimes of strong growth rate$\rho_\text{high}$ and regimes of low growth rate $\rho_\text{low}$. thesuccession of such two regimes compounds to produce a remarkably stable longterm average real annualized growth rate of 1.6\% from 1800 to 2010 and$\approx 2.0\%$ since 1950, which is the result of a subtle compensationbetween the high and low growth regimes that alternate continuously. thus, theoverall growth dynamics of the us economy is punctuated, with phases of stronggrowth that are intrinsically unsustainable, followed by corrections orconsolidation until the next boom starts. we interpret these findings withinthe theory of "social bubbles" and argue as a consequence that estimations ofthe cost of the 2008 crisis may be misleading. we also interpret the absence ofstrong recovery since 2008 as a protracted low growth regime $\rho_\text{low}$associated with the exceptional nature of the preceding large growth regime.
{fenge}
cond-mat;0002075	finite-time singularity in the dynamics of the world population,  economic and financial indices	contrary to common belief, both the earth's human population and its economicoutput have grown faster than exponential, i.e., in a super-malthusian mode,for most of the known history. these growth rates are compatible with aspontaneous singularity occuring at the same critical time 2052 +- 10 signalingan abrupt transition to a new regime. the degree of abruptness can be inferedfrom the fact that the maximum of the world population growth rate was reachedin 1970, i.e., about 80 years before the predicted singular time, correspondingto approximately 4% of the studied time interval over which the acceleration isdocumented. this rounding-off of the finite-time singularity is probably due toa combination of well-known finite-size effects and friction and suggests thatwe have already entered the transition region to a new regime. in theoreticalsupport, a multivariate analysis coupling population, capital, r&amp;d andtechnology shows that a dramatic acceleration in the population during most ofthe timespan can occur even though the isolated dynamics do not exhibit it.possible scenarios for the cross-over and the new regime are discussed.nottale, chaline and grou have recently independently applied a log-periodicanalysis to the main crises of different civilisations. it is striking thatthese two independent analyses based on a different data set gives a criticaltime which is compatible within the error bars.
{fenge}
cond-mat;0003478	critical ruptures	the fracture of materials is a catastrophic phenomenon of considerabletechnological and scientific importance. here, we analysed experiments designedfor industrial applications in order to test the concept that, in heterogeneousmaterials such as fiber composites, rocks, concrete under compression andmaterials with large distributed residual stresses, rupture is a genuinecritical point, i.e. the culmination of a self-organization of damage andcracking characterized by power law signatures. specifically, we analyse theacoustic emissions recorded during the pressurisation of spherical tanks ofkevlar or carbon fibers pre-impregnated in a resin matrix wrapped up around athin metallic liner (steel or titanium) fabricated and instrumented bya\'erospatiale-matra inc. these experiments are performed as part of a routineindustrial procedure which tests the quality of the tanks prior to shipment andvaries in nature. we find that the seven acoustic emission recordings of sevenpressure tanks which was brought to rupture exhibit clear acceleration inagreement with a power law ``divergence'' expected from the critical pointtheory. in addition, we find strong evidence of log-periodic corrections thatquantify the intermittent succession of accelerating bursts and quiescentphases of the acoustic emissions on the approach to rupture. an improved modelaccounting for the cross-over from the non-critical to the critical regionclose to the rupture point exhibits interesting predictive potential.
{fenge}
cond-mat;0010050	large stock market price drawdowns are outliers	drawdowns are essential aspects of risk assessment in investment management.they offer a more natural measure of real market risks than the variance orother cumulants of daily (or some other fixed time scale) distributions ofreturns. here, we extend considerably our previous analysis by analyzing themajor financial indices, the major currencies, gold, the twenty largest u.s.companies in terms of capitalisation as well as nine others chosen randomly. wefind for the major financial markets that approximately 98% of thedistributions of drawdowns is well-represented by a stretched exponential whilethe largest drawdowns are occurring with a significantly larger rate thanpredicted by the stretched exponential: the largest drops thus constitutegenuine outliers. this is confirmed by extensive testing on surrogate data,which unambiguously show that large stock market drops (and crashes) cannot beaccounted for by the distribution of returns characterising the smaller marketmoves. they thus belong to a different class of their own and call for aspecific amplification mechanism. a similar scenario is found for the majorityof the company stocks analysed. drawups exhibit a similar behavior in onlyabout half the markets that we examined. in the spirit of bacon in novumorganum about 400 years ago, ``errors of nature, sports and monsters correctthe understanding in regard to ordinary things, and reveal general forms. forwhoever knows the ways of nature will more easily notice her deviations; and,on the other hand, whoever knows her deviations will more accurately describeher ways,'' we propose that outliers reveal fundamental properties of the stockmarket.
{fenge}
cond-mat;0110436	evidence of intermittent cascades from discrete hierarchical dissipation  in turbulence	we present the results of a search of log-periodic corrections to scaling inthe moments of the energy dissipation rate in experiments at high reynoldsnumber (2500) of three-dimensional fully developed turbulence. a simpledynamical representation of the richardson-kolmogorov cartoon of a cascadeshows that standard averaging techniques erase by their very construction thepossible existence of log-periodic corrections to scaling associated with adiscrete hierarchy. to remedy this drawback, we introduce a novel ``canonical''averaging that we test extensively on synthetic examples constructed to mimickthe interplay between a weak log-periodic component and rather strongmultiplicative and phase noises. our extensive tests confirm the remarkableobservation of statistically significant log-periodic corrections to scaling,with a prefered scaling ratio for length scales compatible with the value gamma= 2. a strong confirmation of this result is provided by the identification ofup to 5 harmonics of the fundamental log-periodic undulations, associated withup to 5 levels of the underlying hierarchical dynamical structure. a naturalinterpretation of our results is that the richardson-kolmogorov mental pictureof a cascade becomes a realistic description if one allows for intermittentbirths and deaths of discrete cascades at varying scales.
{fenge}
cond-mat;0110445	statistical significance of periodicity and log-periodicity with  heavy-tailed correlated noise	we estimate the probability that random noise, of several plausible standarddistributions, creates a false alarm that a periodicity (or log-periodicity) isfound in a time series. we investigate general situations with non-gaussiancorrelated noises and present synthetic tests on the detectability andstatistical significance of periodic components. increasing heavy-tailness(respectively correlations describing persistence) tends to decrease(respectively increase) the false-alarm probability of finding a large spuriouslomb peak. increasing anti-persistence tends to decrease the false-alarmprobability. we also study the interplay between heavy-tailness and long-rangecorrelations. in order to fully determine if a lomb peak signals a genuinerather than a spurious periodicity, one should in principle characterize thelomb peak height, its width and its relations to other peaks in the completespectrum. as a step towards this full characterization, we construct thejoint-distribution of the frequency position (relative to other peaks) and ofthe height of the highest peak of the power spectrum. we also provide thedistributions of the ratio of the second highest lomb peak to the maximum peak.using the insight obtained by the present statistical study, we re-examinepreviously reported claims of ``log-periodicity'' and find that the credibilityfor log-periodicity in 2d-freely decaying turbulence is weakened while it isstrengthened for fracture, for the ion-signature prior to the kobe earthquakeand for financial markets.
{fenge}
cond-mat;0111181	classification of possible finite-time singularities by functional  renormalization	starting from a representation of the early time evolution of a dynamicalsystem in terms of the polynomial expression of some observable f (t) as afunction of the time variable in some interval 0 &lt; t &lt; t, we investigate how toextrapolate/forecast in some optimal stability sense the future evolution off(t) for time t&gt;t. using the functional renormalization of yukalov and gluzman,we offer a general classification of the possible regimes that can be definedbased on the sole knowledge of the coefficients of a second-order polynomialrepresentation of the dynamics. in particular, we investigate the conditionsfor the occurence of finite-time singularities from the structure of the timeseries, and quantify the critical time and the functional nature of thesingularity when present. we also describe the regimes when a smooth extremumreplaces the singularity and determine its position and amplitude. this extendsprevious works by (1) quantifying the stability of the functionalrenormalization method more accurately, (2) introducing new global constraintsin terms of moments and (3) going beyond the ``mean-field'' approximation.
{fenge}
0810.1922	look-ahead benchmark bias in portfolio performance evaluation	performance of investment managers are evaluated in comparison withbenchmarks, such as financial indices. due to the operational constraint thatmost professional databases do not track the change of constitution ofbenchmark portfolios, standard tests of performance suffer from the "look-aheadbenchmark bias," when they use the assets constituting the benchmarks ofreference at the end of the testing period, rather than at the beginning of theperiod. here, we report that the "look-ahead benchmark bias" can exhibit asurprisingly large amplitude for portfolios of common stocks (up to 8% annumfor the s&amp;p500 taken as the benchmark) -- while most studies have emphasizedrelated survival biases in performance of mutual and hedge funds for which thebiases can be expected to be even larger. we use the crsp database from 1926 to2006 and analyze the running top 500 us capitalizations to demonstrate thatthis bias can account for a gross overestimation of performance metrics such asthe sharpe ratio as well as an underestimation of risk, as measured forinstance by peak-to-valley drawdowns. we demonstrate the presence of asignificant bias in the estimation of the survival and look-ahead biasesstudied in the literature. a general methodology to test the properties ofinvestment strategies is advanced in terms of random strategies with similarinvestment constraints.
{fenge}
cond-mat;0503607	importance of positive feedbacks and over-confidence in a  self-fulfilling ising model of financial markets	following a long tradition of physicists who have noticed that the isingmodel provides a general background to build realistic models of socialinteractions, we study a model of financial price dynamics resulting from thecollective aggregate decisions of agents. this model incorporates imitation,the impact of external news and private information. it has the structure of adynamical ising model in which agents have two opinions (buy or sell) withcoupling coefficients which evolve in time with a memory of how past news haveexplained realized market returns. we study two versions of the model, whichdiffer on how the agents interpret the predictive power of news. we show thatthe stylized facts of financial markets are reproduced only when agents areover-confident and mis-attribute the success of news to predict return toherding effects, thereby providing positive feedbacks leading to the modelfunctioning close to the critical point. our model exhibits a rich multifractalstructure characterized by a continuous spectrum of exponents of the power lawrelaxation of endogenous bursts of volatility, in good agreement with previousanalytical predictions obtained with the multifractal random walk model andwith empirical facts.
{fenge}
cond-mat;0605676	exploring self-similarity of complex cellular networks: the  edge-covering method with simulated annealing and log-periodic sampling	song, havlin and makse (2005) have recently used a version of thebox-counting method, called the node-covering method, to quantify theself-similar properties of 43 cellular networks: the minimal number $n_v$ ofboxes of size $\ell$ needed to cover all the nodes of a cellular network wasfound to scale as the power law $n_v \sim (\ell+1)^{-d_v}$ with a fractaldimension $d_v=3.53\pm0.26$. we propose a new box-counting method based onedge-covering, which outperforms the node-covering approach when applied tostrictly self-similar model networks, such as the sierpinski network. theminimal number $n_e$ of boxes of size $\ell$ in the edge-covering method isobtained with the simulated annealing algorithm. we take into account thepossible discrete scale symmetry of networks (artifactual and/or real), whichis visualized in terms of log-periodic oscillations in the dependence of thelogarithm of $n_e$ as a function of the logarithm of $\ell$. in this way, weare able to remove the bias of the estimator of the fractal dimension, existingfor finite networks. with this new methodology, we find that $n_e$ scales withrespect to $\ell$ as a power law $n_e \sim \ell^{-d_e}$ with $d_e=2.67\pm0.15$for the 43 cellular networks previously analyzed by song, havlin and makse(2005). bootstrap tests suggest that the analyzed cellular networks may have asignificant log-periodicity qualifying a discrete hierarchy with a scalingratio close to 2. in sum, we propose that our method of edge-covering withsimulated annealing and log-periodic sampling minimizes the significant bias inthe determination of fractal dimensions in log-log regressions.
{fenge}
cond-mat;9510032	faults self-organized by repeated earthquakes in a quasi-static  antiplane crack model	we study a 2d quasi-static discrete {\it crack} anti-plane model of atectonic plate with long range elastic forces and quenched disorder. the plateis driven at its border and the load is transfered to all elements throughelastic forces. this model can be considered as belonging to the class ofself-organized models which may exhibit spontaneous criticality, with fouradditional ingredients compared to sandpile models, namely quenched disorder,boundary driving, long range forces and fast time crack rules. in this''crack'' model, as in the ''dislocation'' version previously studied, we findthat the occurrence of repeated earthquakes organizes the activity onwell-defined fault-like structures. in contrast with the ''dislocation'' model,after a transient, the time evolution becomes periodic with run-aways endingeach cycle. this stems from the ''crack'' stress transfer rule preventingcriticality to organize in favor of cyclic behavior. for sufficiently largedisorder and weak stress drop, these large events are preceded by a complexspace-time history of foreshock activity, characterized by a gutenberg-richterpower law distribution with universal exponent $b=1 \pm 0.05$. this is similarto a power law distribution of small nucleating droplets before the nucleationof the macroscopic phase in a first-order phase transition. for large disorderand large stress drop, and for certain specific initial disorderconfigurations, the stress field becomes frustrated in fast time : out-of-planedeformations (thrust and normal faulting) and/or a genuine dynamics must beintroduced to resolve this frustration.
{fenge}
cond-mat;9510035	rank-ordering statistics of extreme events: application to the  distribution of large earthquakes	rank-ordering statistics provides a perspective on the rare, largest elementsof a population, whereas the statistics of cumulative distributions aredominated by the more numerous small events. the exponent of a power lawdistribution can be determined with good accuracy by rank-ordering statisticsfrom the observation of only a few tens of the largest events. using analyticalresults and synthetic tests, we quantify the systematic and the random errors.  we also study the case of a distribution defined by two branches, each havinga power law distribution, one defined for the largest events and the other forsmaller events, with application to the world-wide (harvard) and southerncalifornia earthquake catalogs. in the case of the harvard moment catalog, wemake more precise earlier claims of the existence of a transition of theearthquake magnitude distribution between small and large earthquakes; the$b$-values are $b_2 = 2.3 \pm 0.3$ for large shallow earthquakes and $b_1 =1.00 \pm 0.02$ for smaller shallow earthquakes. however, the cross-overmagnitude between the two distributions is ill-defined. the data available atpresent do not provide a strong constraint on the cross-over which has a $50\%$probability of being between magnitudes $7.1$ and $7.6$ for shallowearthquakes; this interval may be too conservatively estimated. thus, anyinfluence of a universal geometry of rupture on the distribution of earthquakesworld-wide is ill-defined at best. we caution that there is no direct evidenceto confirm the hypothesis that the large-moment branch is indeed a power law.in fact, a gamma distribution fits the entire suite of earthquake moments fromthe smallest to the largest satisfactorily. there is no evidence that theearthquakes of the southern california catalog have a distribution with two
{fenge}
cond-mat;9609074	convergent multiplicative processes repelled from zero: power laws and  truncated power laws	random multiplicative processes $w_t =\lambda_1 \lambda_2 ... \lambda_t$(with &lt; \lambda_j &gt; 0 ) lead, in the presence of a boundary constraint, to adistribution $p(w_t)$ in the form of a power law $w_t^{-(1+\mu)}$. we provide asimple and physically intuitive derivation of this result based on a randomwalk analogy and show the following: 1) the result applies to the asymptotic($t \to \infty$) distribution of $w_t$ and should be distinguished from thecentral limit theorem which is a statement on the asymptotic distribution ofthe reduced variable ${1 \over \sqrt{t}}(log w_t -&lt; log w_t &gt;)$; 2) thenecessary and sufficient conditions for $p(w_t)$ to be a power law are that&lt;log \lambda_j &gt; &lt; 0 (corresponding to a drift $w_t \to 0$) and that $w_t$ notbe allowed to become too small. we discuss several models, previouslyunrelated, showing the common underlying mechanism for the generation of powerlaws by multiplicative processes: the variable $\log w_t$ undergoes a randomwalk biased to the left but is bounded by a repulsive ''force''. we give anapproximate treatment, which becomes exact for narrow or log-normaldistributions of $\lambda$, in terms of the fokker-planck equation. 3) for allthese models, the exponent $\mu$ is shown exactly to be the solution of$\langle \lambda^{\mu} \rangle = 1$ and is therefore non-universal and dependson the distribution of $\lambda$.
{fenge}
cond-mat;9707012	discrete scale invariance and complex dimensions	we discuss the concept of discrete scale invariance and how it leads tocomplex critical exponents (or dimensions), i.e. to the log-periodiccorrections to scaling. after their initial suggestion as formal solutions ofrenormalization group equations in the seventies, complex exponents have beenstudied in the eighties in relation to various problems of physics embedded inhierarchical systems. only recently has it been realized that discrete scaleinvariance and its associated complex exponents may appear ``spontaneously'' ineuclidean systems, i.e. without the need for a pre-existing hierarchy. examplesare diffusion-limited-aggregation clusters, rupture in heterogeneous systems,earthquakes, animals (a generalization of percolation) among many othersystems. we review the known mechanisms for the spontaneous generation ofdiscrete scale invariance and provide an extensive list of situations wherecomplex exponents have been found. this is done in order to provide a basis fora better fundamental understanding of discrete scale invariance. the mainmotivation to study discrete scale invariance and its signatures is that itprovides new insights in the underlying mechanisms of scale invariance. it mayalso be very interesting for prediction purposes.
{fenge}
cond-mat;9801326	self-organized critical random directed polymers	we uncover a nontrivial signature of the hierarchical structure ofquasi-degenerate random directed polymers (rdps) at zero temperature in 1+1dimensional lattices. using a cylindrical geometry with circumference $8 \leq w\leq 512$, we study the differences in configurations taken by rdps forced topass through points displaced successively by one unit lattice mesh. thetransition between two successive configurations (interpreted as an avalanche)defines an area $s$. the distribution of moderatly sized avalanches is found tobe a power-law $p(s) ds \sim s^{-(1+\mu)} ds$. using a hierarchical formulationbased on the length scales $w^{2\over 3}$ (transverse excursion) and thedistance $w^{{2\over 3}\alpha}$ between quasi-degenerate ground states (with$0&lt;\alpha\le 1$), we determine $\mu = {2\over 5}$, in excellent agreement withnumerical simulations by a transfer matrix method. this power-law is valid upto a maximum size $s_{5\over 3} \sim w^{5\over 3}$. there is another populationof avalanches which, for characteristic sizes beyond $s_{5\over 3}$, obeys$p(s) ds \sim \exp(-(s/s_{5\over 3})^3) ds$ also confirmed numerically. thefirst population corresponds to almost degenerate ground states, providing adirect evidence of ``weak replica symmetry breaking'', while the secondpopulation is associated with different optimal states separated by the typicalfluctuation $w^{2\over 3}$ of a single rdp.
{fenge}
cond-mat;9802059	large deviations and portfolio optimization	risk control and optimal diversification constitute a major focus in thefinance and insurance industries as well as, more or less consciously, in oureveryday life. we present a discussion of the characterization of risks and ofthe optimization of portfolios that starts from a simple illustrative model andends by a general functional integral formulation. a major theme is that risk,usually thought one-dimensional in the conventional mean-variance approach, hasto be addressed by the full distribution of losses. furthermore, thetime-horizon of the investment is shown to play a major role. we show theimportance of accounting for large fluctuations and use the theory of cram\'erfor large deviations in this context. we first treat a simple model with asingle risky asset that examplifies the distinction between the average returnand the typical return, the role of large deviations in multiplicativeprocesses, and the different optimal strategies for the investors depending ontheir size. we then analyze the case of assets whose price variations aredistributed according to exponential laws, a situation that is found todescribe reasonably well daily price variations. several portfolio optimizationstrategies are presented that aim at controlling large risks. we end byextending the standard mean-variance portfolio optimization theory, firstwithin the quasi-gaussian approximation and then using a general formulationfor non-gaussian correlated assets in terms of the formalism of functionalintegrals developed in the field theory of critical phenomena.
{fenge}
cond-mat;9809366	economic returns of research: the pareto law and its implications	at what level should government or companies support research? this complexmulti-faceted question encompasses such qualitative bonus as satisfying naturalhuman curiosity, the quest for knowledge and the impact on education andculture, but one of its most scrutinized component reduces to the assessment ofeconomic performance and wealth creation derived from research. in certainareas such as biotechnology, semi-conductor physics, optical communications,the impact of basic research is direct while, in other disciplines, the pathfrom discovery to applications is full of surprises. as a consequence, thereare persistent uncertainties in the quantification of the exact economicreturns of public expenditure on basic research. here, we suggest that theseuncertainties have a fundamental origin to be found in the interplay betweenthe intrinsic ``fat tail'' power law nature of the distribution of economicreturns, characterized by a mathematically diverging variance, and thestochastic character of discovery rates. in the regime where the cumulativeeconomic wealth derived from research is expected to exhibit a long-termpositive trend, we show that strong fluctuations blur out significantly theshort-time scales: a few major unpredictable innovations may provide a finitefraction of the total creation of wealth. in such a scenario, any attempt toassess the economic impact of research over a finite time horizon encompassingonly a small number of major discoveries is bound to be highly unreliable. newtools, developed in the theory of self-similar and complex systems to tacklesimilar extreme fluctuations in nature can be adapted to measure the economicbenefits of research, which is intimately associated to this large variability.
{fenge}
cond-mat;9810071	crashes as critical points	we study a rational expectation model of bubbles and crashes. the model hastwo components : (1) our key assumption is that a crash may be caused by localself-reinforcing imitation between noise traders. if the tendency for noisetraders to imitate their nearest neighbors increases up to a certain pointcalled the ``critical'' point, all noise traders may place the same order(sell) at the same time, thus causing a crash. the interplay between theprogressive strengthening of imitation and the ubiquity of noise ischaracterized by the hazard rate, i.e. the probability per unit time that thecrash will happen in the next instant if it has not happened yet. (2) since thecrash is not a certain deterministic outcome of the bubble, it remains rationalfor traders to remain invested provided they are compensated by a higher rateof growth of the bubble for taking the risk of a crash. our model distinguishesbetween the end of the bubble and the time of the crash,: the rationalexpectation constraint has the specific implication that the date of the crashmust be random. the theoretical death of the bubble is not the time of thecrash because the crash could happen at any time before, even though this isnot very likely. the death of the bubble is the most probable time for thecrash. there also exists a finite probability of attaining the end of thebubble without crash. our model has specific predictions about the presence ofcertain critical log-periodic patterns in pre-crash prices, associated with thedeterministic components of the bubble mechanism. we provide empirical evidenceshowing that these patterns were indeed present before the crashes of 1929,1962 and 1987 on wall street and the 1997 crash on the hong kong stockexchange. these results are compared with statistical tests on synthetic data.
{fenge}
cond-mat;9907270	log-periodic power law bubbles in latin-american and asian markets and  correlated anti-bubbles in western stock markets: an empirical study	twenty-two significant bubbles followed by large crashes or by severecorrections in the argentinian, brazilian, chilean, mexican, peruvian,venezuelan, hong-kong, indonesian, korean, malaysian, philippine and thai stockmarkets indices are identified and analysed for log-periodic signaturesdecorating an average power law acceleration. we find that log-periodic powerlaws adequately describe speculative bubbles on these emerging markets withvery few exceptions and thus extend considerably the applicability of theproposed rational expectation model of bubbles and crashes which has previouslybeen developed for the major financial markets in the world. this model isessentially controlled by a crash hazard rate becoming critical due to acollective imitative/herding behavior of traders. furthermore, three of thebubbles are followed by a log-periodic ``anti-bubble'' previously documentedfor the decay of the japanese nikkei starting in jan. 1990 and the price ofgold starting in sept. 1980 thus rendering a qualitative symmetry of bubble andanti-bubble around the date of the peak of the market. a set of secondarywestern stock market indices (london, sydney, auckland, paris, madrid, milan,zurich) as well as the hong-kong stock market are also shown to exhibitwell-correlated log-periodic power law anti-bubbles over a period 6-15 monthstriggered by a rash of crises on emerging markets in the early 1994. as the usmarket declined by no more than 10% during the beginning of that period andquickly recovered, this suggests that these smaller stock western markets can``phase lock'' (in a weak sense) not only because of the over-arching influenceof wall street but also independently of the current trends on wall street dueto other influences.
